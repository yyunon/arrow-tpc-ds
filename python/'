import pyarrow as pa
from pyarrow import csv

# This model will hold the dataset and schema information.
 
class dataset:
    def __init__(self, c_prefix=None, metadata_information=None):
        self.metadata = metadata_information
        self.database_name=None
        self.prefix= None
        self.recordbatch_name=None
        self.table=None
        self.data=None
        self.schema=None
        self.recordbatch=None
        self.writer=None
        self.opt = csv.ParseOptions()
        if c_prefix == None:
            import os
            for k,v in os.environ.items():
                if k =='DATASET_DIR':
                    self.prefix = v
            print(f"Using auto environment variable: {self.prefix} ")
        else:
            #TODO: Write later on
            assert(False)
    @property
    def options(self):
        return self.opt
    @options.setter
    def options(self, d="|"):
        self.opt.delimeter(d)
    def stream_to_file(self):
        pass
    def print_col(self,column):
        assert(self.data != None)
        print(f"Column: {column}, Row:{self.data[column]} ")

class store_sales(dataset):

    def __init__(self, columns=None, c_prefix=None,metadata_information=None,field_metadata=None,metadata_indexes=None):
        if metadata_information == None or metadata_indexes==None:
            assert(False)
        super().__init__(c_prefix, metadata_information)
        self.ss_type = ['int64','int64','int64','int64','int64','float64','float64']
        self.ss_sold_date_sk = pa.field('ss_sold_date_sk', pa.int64(), nullable=False)
        self.ss_cdemo_sk= pa.field('ss_cdemo_sk', pa.int64(), nullable=False)
        self.ss_addr_sk= pa.field('ss_addr_sk', pa.int64(), nullable=False)
        self.ss_store_sk= pa.field('ss_store_sk', pa.int64(), nullable=False)
        self.ss_quantity= pa.field('ss_quantity', pa.int64(), nullable=False)
        self.ss_sales_price=pa.field('ss_sales_price', pa.float64(), nullable=False)
        self.ss_net_profit=pa.field('ss_net_profit', pa.float64(), nullable=False)
        # Configure schema
        #TODO : make this general
        self.schema = pa.schema([self.ss_sold_date_sk, 
                                 self.ss_cdemo_sk,
                                 self.ss_addr_sk, 
                                 self.ss_store_sk,
                                 self.ss_quantity, 
                                 self.ss_sales_price, 
                                 self.ss_net_profit])
        self.schema.add_metadata(metadata_information)
        # Field metadata such as epc, not always applicable
        self.field_metadata = field_metadata
        # output file names
        self.recordbatch_name="ss_recordbatch.rb"
        self.database_name = "store_sales.dat"
        if columns == None:
            print("Reading all columns...")
            self.table = csv.read_csv(self.prefix + self.database_name,self.opt)
            for i,_ in enumerate(table):
                if i in columns: 
                    self.data.append(pa.array(table.column(i).to_pylist()))
        else: 
            print("Reading all columns : {columns}")
            self.table = csv.read_csv(self.prefix + self.database_name,self.opt)
            for i,_ in enumerate(table):
                if i in columns: 
                    self.data.append(pa.array(table.column(i).to_pylist()))
    def stream_to_file(self):
        self.recordbatch = pa.RecordBatch.from_arrays(self.data, schema=schema)

        # Create an Arrow RecordBatchFileWriter.
        self.writer = pa.RecordBatchFileWriter(self.prefix + "/" + ss.recordbatch_name, schema)

        # Write the RecordBatch.
        self.writer.write(recordbatch)

        # Close the writer.
        writer.close()



